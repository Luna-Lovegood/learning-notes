%-*- coding: UTF-8 -*-
% notes.tex
%
\documentclass[UTF8]{article}
\usepackage{geometry}
\geometry{a4paper, centering, scale=0.8}
\usepackage{minted}
\usepackage{hyperref}
\usepackage{indentfirst}    % to indent the first paragraph of a section
\usepackage{graphicx}       % to insert figures
\usepackage{amsmath}        % to type some math equations
\usepackage{amssymb}        % to use some special math font
\usepackage{IEEEtrantools}  % to use IEEEeqnarray
\usepackage{algorithm2e}    % to use algorithm environment
\usepackage{multicol}       % to display some content in multi-columns
\setlength{\columnseprule}{0.4pt}   % set the rule's width of multicols
\setlength{\columnsep}{5em}         % set the sep of multicols

% Math notation
% refered to https://github.com/exacity/deeplearningbook-chinese/blob/master/math_symbol.tex
\newcommand{\Scalar}[1]{\mathit{#1}}                % Scalar, the default math font
\newcommand{\Vector}[1]{\boldsymbol{\mathit{#1}}}   % Vector
\newcommand{\Matrix}[1]{\boldsymbol{\mathit{#1}}}   % Matrix
\newcommand{\Tensor}[1]{\textsf{\textbf{#1}}}       % Tensor
\newcommand{\Set}[1]{\mathbb{#1}}                   % Set
\newcommand{\Cal}[1]{\mathcal{#1}}                  % Math Cal

% Draw the lines in a matrix, which is composed by a series of vectors
\newcommand{\vRule}{\rule{0.3pt}{10mm}}             % vertical rule
\newcommand{\hRule}{\,\rule[1mm]{10mm}{0.3pt}\,}    % horizontal rule

\title{Deep Learning Specialization \\
        Structuring Machine Learning Projects}
\author{Du Ang \\ \texttt{du2ang233@gmail.com} }
\date{\today}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{ML Strategy}
\subsection{Introduction to ML Strategy}
\subsubsection{Why ML Strategy?}
90\% is not good enough for your application, so you might try the following ideas to improve:
\begin{itemize}
    \item Collect more data
    \item Collect more diverse training set
    \item Train algorithm longer with gradient descent
    \item Try Adam instead of gradient descent
    \item Try bigger network
    \item Tray small network
    \item Try dropout
    \item Add $L_2$ regularization
    \item Network architecture
    \begin{itemize}
        \item Activation functions
        \item \# hidden units
        \item \ldots
    \end{itemize}
\end{itemize}

When trying to improve a deep learning system, you often have a lot of ideas or things you could
try, ML strategy will offer you quick and effective ways to figure out which of all of these ideas
are worth pursuing.

\subsubsection{Orthogonalization}
Orthogonalization or orthogonality is a system design property that assures that modifying an
instruction or a component of an algorithm will not create or propagate side effects to other
components of the system. It becomes easier to verify the algorithms independently from one another,
it reduces testing and development time.

When a supervised learning system is design, these are the 4 assumptions that needs to be true and
orthogonal.
\begin{enumerate}
    \item Fit training set well in cost function
    \begin{itemize}
        \item If it doesn’t fit well, the use of a bigger neural network or switching to a better
        optimization algorithm might help.
    \end{itemize}
    \item Fit development set well on cost function
    \begin{itemize}
        \item If it doesn’t fit well, regularization or using bigger training set might help.
    \end{itemize}
    \item Fit test set well on cost function
    \begin{itemize}
        \item If it doesn't fit well, the use of a bigger development set might help
    \end{itemize}
    \item Performs well in real world
    \begin{itemize}
        \item If it doesn't perform well, the development test set is not set correctly or the cost
        function is not evaluating the right thing.
    \end{itemize}
\end{enumerate}

\subsection{Setting up your goal}
\subsubsection{Single number evaluation metric}
To choose a classifier, a well-defined development set and an evaluation metric speed up the
iteration process.

\begin{figure}[htb]
    \centering
    \includegraphics[width=25em]{figures/binary-confusion-matrix}
    \caption{Example: Cat vs Non-cat. $y$ = 1, cat image detected.}
\end{figure}

\paragraph{Precision}
Of all the images we predicted $y = 1$, what fraction of it have cats?

$$ \text{Precision(\%)} = \frac{Ture positive}{Number of predicted positive} \times 100
= \frac{Ture positive}{True positives + False positives} \times 100 $$

\paragraph{Recall}
Of all the images that actually have cats, what fraction of it did we correctly identifying have
cats?

$$ \text{Recall(\%)} = \frac{True positive}{Number of predicted actually positive} \times 100
= \frac{Ture positive}{True positive + False negative} \times 100 $$

The problem with using precision/recall as the evaluation metric is that you are not sure which one
is better since in this case, both of them have a good precision et recall. F1-score, a harmonic
mean, combine both precision and recall.

$$ F_1\text{ score} = \frac{2}{\frac{1}{p} + \frac{1}{r}} $$

\subsubsection{Satisficing and Optimizing metric}
There are different metrics to evaluate the performance of a classifier, they are called evaluation
matrics. They can be categorized as satisficing and optimizing matrics. It is important to note
that these evaluation matrics must be evaluated on a training set, a development set or on the
test set.

\begin{figure}[htb]
    \centering
    \includegraphics[width=25em]{figures/satisficing}
    \caption{Example: Cat vs Non-cat, accuracy and running time are the evaluation metrics.}
\end{figure}

Accuracy is the optimizing metric, because you want the classifier to correctly detect a cat image
as accuracy as possible. The running time which is set to be under 100ms in this example, is the
satisficing metric which mean that the metric has to meet expectation set.

The general rule is
$$ N_{\text{metric}}: \left\{\begin{array}{ll} 1 & \text{Optimizing metric} \\ N_{\text{metric}}-1
& \text{Satisficing metric} \end{array}\right. $$

\subsubsection{Train/dev/test distributions}
Setting up the training, development and test sets have a huge impact on productivity. It is
important to choose the development and test sets from the same distribution and it must be taken
randomly from all the data.

\paragraph{Guideline} Choose a development set and test set to reflect data you expect to get in
the future and consider important to do well.

\subsubsection{Size of the dev and test sets}
\paragraph{Old way of splitting data}
We had smaller dataset therefore we had to use a greater percentage of data to develop and test
ideas and models.

\begin{figure}[htb]
    \centering
    \includegraphics[width=30em]{figures/old-ways-splitting-data}
    \caption{Old ways of splitting data}
    \label{fig:old-ways-splitting-data}
\end{figure}

\paragraph{Modern era --- Big data}
Now, because a large amount of data is available, we don't have to compromised as much and can use
a greater portion to train the model.

\begin{figure}[htb]
    \centering
    \includegraphics[width=30em]{figures/modern-era-splitting-data}
    \caption{The way of splitting data in big data era}
    \label{fig:modern-era-splitting-data}
\end{figure}

Guidelines
\begin{itemize}
    \item Set up the size of test set to give a high confidence in the overall performance of the
    system.
    \item Test set helps evaluate the performance of the final classifier which could be less 30\%
    of the whole dataset.
    \item The development set has to be big enough to evaluate different ideas.
\end{itemize}

\subsubsection{When to change dev/test sets and metrics}
\paragraph{Example: Cat vs Non-cat}
A cat classifier tries to find a great amount of cat images to show to to cat loving users. The
evaluation metric used is a classification error, as it is shown in
Figure~\ref{fig:classification-error}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=25em]{figures/classification-error}
    \caption{The classification error of Algorithm A and Algorithm B}
    \label{fig:classification-error}
\end{figure}

It seems that Algorithm A is better than Algorithm B since there is only a 3\% error, however for
some reason, Algorithm A is letting through a lot of pornographic images.

Algorithm B has 5\% error thus it classifies fewer images but it doesn't have pornographic images.
From a company's point of view, as well as from a user acceptance point of view, Algorithm B is
actually a better algorithm. The evaluation metric fails to correctly rank order preferences
between algorithms. The evaluation metric or the development set or test set should be charged.

The misclassification error can be written as a function as follows:
$$ Error: \frac{1}{m_{dev}}\sum_{i=1}^{m_{dev}} \Cal{L}\{\hat{y}^{(i)}\neq y^{(i)}\} $$

This function counts up the nubmer of misclassified examples.

The problem with this evaluation metric is that it treats pornographic vs non-pornographic images
equally. On way to change this evaluation metrics is to add the weight term $w^{(i)}$.
$$ w^{(i)} = \left\{\begin{array}{rl} 1 & \text{if } x^{(i)} \text{ is non-pornographic} \\
10 & \text{if } x^{(i)} \text{ is pornographic} \end{array}\right. $$

The function becomes:
$$ Error: \frac{1}{\sum_w^{(i)}} \sum_{i=1}^{m_{dev}} w^{(i)} \Cal{L}\{\hat{y}^{(i)} \neq y^{(i)}
\} $$

\paragraph{Guideline}
\begin{enumerate}
    \item Define correctly an evaluation metric that helps better rank order classifiers
    \item Optimize the evaluation metric
    \item If doing well on your metric + dev/test set does not correspond to doing well on your
    application, change your metric and/or dev/test set.
\end{enumerate}

\subsection{Comparing to human-level performance}
\subsubsection{Why human-level performance?}
Today, machine learnning algorithms can compete with human-level performance since they are more
productive and more feasible in a lot of application. Also, the workflow of designing and building
a machine learning system, is much more efficient than before.

Moreover, some of the tasks that humans do are close to ``perfection'', which is why machine
learning tries to mimic human-level performance.

Figure~\ref{fig:human-level} shows the performance of humans and machine learning over time.

\begin{figure}[htb]
    \centering
    \includegraphics[width=40em]{figures/human-level}
    \caption{The performance of humans and machine learning over time.}
    \label{fig:human-level}
\end{figure}

Machine learning progresses slowly when it surpasses human-level performance. One of the reason is
that human-level performance can be close to Bayes optimal error, especially for natural perception
problem.

Bayes optimal error is defined as the best possible error. In other words, it means that any
functions mapping from $x$ to $y$ can't surpass a certain level of accuracy.

Also, when the performance of machine learning is worse than the performance of humans, you can
improve it with different tools. They are harder to use once it's surpasses human-level performance.
These tools are:
\begin{itemize}
    \item Get labeled data from humans
    \item Gain insight from manual error analysis: Why did a person get this right?
    \item Better analysis of bias/variance.
\end{itemize}

\subsubsection{Avoidable bias}
By knowing what the human-level performance is, it is possible to tell when a training set is
performing well or not.

\begin{figure}[htb]
    \centering
    \includegraphics[width=25em]{figures/avoidable-bias}
    \caption{Different human-level in different scenarios}
    \label{fig:avoidable-bias}
\end{figure}

In the case that Figure~\ref{fig:avoidable-bias} shows, the human-level error as a proxy for
Bayes error since humans are good to identify images. If you want to improve the performance of the
training set but you can't do better than the Bayes error otherwise the training set is overfitting.
By knowing the Bayes error, it is easier to focus on whether bias or variance avoidance tactics
will improve the performance of the model.

\paragraph{Scenario A}
There is a 7\% gap between the performance of the training set and the human-level error. It means
that the algorithm isn't fitting well with the training set since the target is around 1\%. To
resolve the issue, we use bias reduction technique such as training a bigger neural networkk or
running the training set longer.

\paragraph{Scenario B}
The training set is doing good since there is only a 0.5\% difference with the human-level error.
The difference between the training set and the human-level error is called avoidable bias. The
focus here is to reduce the variance since the difference between the training set error and the
development error is 2\%. To resolve the issue, we use variance reduction technique such as
regularization or have a bigger training set.

\subsubsection{Understanding human-level performance}
Human-level error gives an estimate of Bayes error.

\paragraph{Example 1: Medical image classification}
\begin{figure}[htb]
    \centering
    \includegraphics[width=25em]{figures/medical-image-classification}
    \caption{Medical image classification}
    \label{fig:medical-image-classification}
\end{figure}

This is an example of a medical image classification in which the input is a radiology image and
the output is a diagnosis classfication decision.

The definition of human-level error depends on the purpose of the analysis, in this case,
by definition the Bayes error is lower or equal to 0.5\%.

\paragraph{Example 2: Error analysis}

\begin{figure}[htb]
    \centering
    \includegraphics[width=30em]{figures/error-analysis}
    \caption{Error analysis in different scenarios according to human-levels}
    \label{fig:error-analysis}
\end{figure}

\subparagraph{Scenario A}
In this case, the choice of human-level performance doesn't have an impact. The avoidable bias is
between 4\%-4.5\% and the variance is 1\%. Therefore, the focus should be on bias reduction
technique.

\subparagraph{Scenario B}
In this case, the choice of human-level performance doesn't have an impact. The avoidable bias is
between 0\%-0.5\% and the variance is 4\%. Therefore, the focus should be on variance reduction
technique.

\subparagraph{Scenario C}
In this case, the estimate for Bayes error has to be 0.5\% since you can't go lower than the
human-level performance otherwise the training set is overfitting. Also, the avoidable bias is
0.2\% and the variance is 0.1\%. Therefore, the focus should be bias reduction technique.

\paragraph{Summary of bais/variance with human-level performance}
\begin{itemize}
    \item Human-level error --- proxy of Bayes error
    \item If the difference between human-level error and the training error is bigger than the
    difference between the training error and the development error. The focus should be on bias
    reduction technique
    \item If the difference between training error and the development error is bigger than the
    difference between the human-level error and the training error. The focus should be on
    variance reduction technique.
\end{itemize}

\subsubsection{Surpassing human-level performance}
\paragraph{Example 1: Classification task}
\begin{figure}[htb]
    \centering
    \includegraphics[width=30em]{figures/surpass-human-level}
    \caption{Classification task}
    \label{fig:surpass-human-level}
\end{figure}

\paragraph{Scenario A}
In this case, the Bayes error is 0.5\%, therefore the available bias is 0.1\% et the variance is
0.2\%.

\paragraph{Scenario B}
In this case, there is not enough information to know if bias reduction or variance reduction has
to be done on the algorithm. It doesn't mean that the model cannot be improve, it means that the
conventional ways to know if bias reduction or variance reduction are not working in this case.

There are many problems where machine learning significantly surpass human-level performance,
especially with structured data:
\begin{itemize}
    \item Online advertising
    \item Product recommendations
    \item Logistics (predicting transmit time)
    \item Loan approvals
\end{itemize}

\subsubsection{Improving your model performance}
\paragraph{The two fundamental assumptions of supervised learning}
There are 2 fundamental assumptions of supervised learning. The first one is to have a low
avoidable bias which means that the training set fits well. The second one is to have a low or
acceptable variance which means that the training set performance generalizes well to the
development set and test set. The strategy of improving your model can be seen in
Figure~\ref{fig:improve-model}.

If the different between human-level error and the training error is bigger than the difference
between the training error and the development error, the focus should be on bias reduction
technique which are training a bigger model, training longer or change the neural networks
architecture or try various hyperparameters search.

If the difference between training error and the development error is bigger than the difference
between the human-level error and the training error, the focus should be on variance reduction
technique which are bigger dataset, regularization or change the neural networks architectures or
try various hyperparameters search.

\begin{figure}[htb]
    \centering
    \includegraphics[width=40em]{figures/improve-model}
    \caption{The strategy to improve your models.}
    \label{fig:improve-model}
\end{figure}

\subsection{Error Analysis}
\subsubsection{Carrying out error analysis}
\paragraph{Look at dev examples to evaluate ideas}
Should you try to make your cat classifier do better on
dogs?

Error analysis:
\begin{itemize}
    \item Get $\sim$100 mislabeled dev set examples.
    \item Count up how many are dogs.
\end{itemize}

\paragraph{Evaluate multiple ideas in parallel}
Ideas for cat detection:
\begin{itemize}
    \item Fix pictures of dogs being recognized as cats
    \item Fix great cats (lions, panthers, etc\ldots) being misrecognized
    \item Improve performance on blurry images
\end{itemize}

\subsubsection{Cleaning up incorrectly labeled data}
DL algorithms are quite robust to random errors in the training set.

Look insight of the overall dev set errors, and compare between the errors due to incorrect labels
and the errors due to other causes. The goal of dev set is to help you select between classifier A
\& B.

\paragraph{Correcting incorrect dev/test set examples}
\begin{itemize}
    \item Apply same process to your dev and test sets to make sure they continue to come from the
    same distribution.
    \item Consider examining examples your algorithm got right as well as ones it got wrong.
    \item Train and dev/test data may now come from slightly different distributions.
\end{itemize}

\subsubsection{Build your first system quickly, then iterate}
Depending on the area of application, the guideline below will help you prioritize when you build
your system.

\paragraph{Guideline}
\begin{enumerate}
    \item Set up development/test set and metrics
    \begin{itemize}
        \item Set up a target
    \end{itemize}
    \item Build an initial system quickly
    \begin{itemize}
        \item Train training set quickly: Fit the parameters
        \item Development set: Tune the parameters
        \item Test set: Assess the performance
    \end{itemize}
    \item Use Bias/Variance analysis \& Error analysis to prioritize next steps
\end{enumerate}

\subsection{Mismatched training and dev/test set}
\subsubsection{Training and testing on different distributions}
\paragraph{Example Cat vs Non-cat}
In this example, we want to create a mobile application that will classify and recognize pictures
of cats taken and uploaded by users.

There are two sources of data used to develop the mobile app. The first data distribution is small,
10,000 pictures uploaded from the mobile application. Since they are from amateur users, the
pictures are not professionally shot, not well framed and blurrier. The second source is from the
web, you downloaded 200,000 pictures where cat’s pictures are professionally framed and in high
resolution.

The problem is that you have a different distribution:
\begin{enumerate}
    \item small data set from pictures uploaded by users. This distribution is important for the
    mobile app.
    \item bigger data set from the web.
\end{enumerate}

The guideline used is that you have to choose a development set and test set to reflect data you
expect to get in the future and consider important to do well.

The data is split as the Figure~\ref{fig:split-data-with-two-diff-sources}. The advantage of this
way of splitting up is that the target is well defined.

The disadvantage is that the training distribution is different from the development and test set
distributions. However, this way of splitting the data has a better performance in long term.

\begin{figure}[htb]
    \centering
    \includegraphics[width=40em]{figures/split-data-with-two-diff-sources}
    \caption{The split of the data, which have }
    \label{fig:split-data-with-two-diff-sources}
\end{figure}

\subsubsection{Bias and Variance with mismatched data distributions}
\paragraph{Example: Cat classifier with mismatched data distribution}
When the training set is from a different distribution than the development and test sets, the
method to analyze bias and variance changes, see
Figure~\ref{fig:bias-variance-with-mismatched-data}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=50em]{figures/bias-variance-with-mismatched-data}
    \caption{Bias and variance with mismatched data.}
    \label{fig:bias-variance-with-mismatched-data}
\end{figure}

\paragraph{Scenario A}
If the development data comes from the same distribution as the training set, then there is a large
variance problem and the algorithm is not generalizing well from the training set.

However, since the training data and the development data come from a different distribution, this
conclusion cannot be drawn. There isn't necessarily a variance problem. The problem might be that
the development set contains images that are more difficult to classify accurately.

When the training set, development and test sets distributions are different, two things change at
the same time. First of all, the algorithm trained in the training set but not in the development
set. Second of all, the distribution of data in the development set is different. It's difficult to
know which of these two changes what produces this 9\% increase in error between the training set
and the development set. To resolve this issue, we define a new subset called training-development
set. This new subset has the same distribution as the training set, but it is not used for training
the neural network.

\paragraph{Scenario B}
The error between the training set and the training-development set is 8\%. In this case, since the
training set and training-development set come from the same distribution, the only difference
between them is the neural network sorted the data in the training and not in the training
development. The neural network is not generalizing well to data from the same distribution that it
hadn't seen before. Therefore, we have really a variance problem.

\paragraph{Scenario C}
In this case, we have a mismatch data problem since the 2 data sets come from different
distribution.

\paragraph{Scenario D}
In this case, the avoidable bias is high since the difference between Bayes error and training
error is 10\%.

\paragraph{Scenario E}
In this case, there are 2 problems. The first one is that the avoidable bias is high since the
difference between Bayes error and training error is 10\% and the second one is a data mismatched
problem.

\paragraph{Scenario F}
Development should never be done on the test set. However, the difference between the development
set and the test set gives the degree of overfitting to the development set.

Figure~\ref{fig:data-mismatched-general-formulation} shows the general formulation.

\begin{figure}[htb]
    \centering
    \includegraphics[width=40em]{figures/data-mismatched-general-formulation}
    \caption{The general formulation of bias and variance with mismatched data.}
    \label{fig:data-mismatched-general-formulation}
\end{figure}

\subsubsection{Addressing data mismatch}
This is a general guideline to address data mismatch:
\begin{itemize}
    \item Perform manual error analysis to understand the error differences between training,
    development/test sets. Development should never be down on test set to avoid overfitting.
    \item Make training data or collect data similar to development and test sets. To make the
    training data more similar to your development set, you can use is artificial data synthesis.
    However, it is possible that if you might be accidentally simulating data only from a tiny
    subset of the space all possible examples.
\end{itemize}

\subsection{Learning from multiple tasks}
\subsubsection{Transfer learning}
Transfer learning refers to using the neural network knowledge from for another application.

When to use transfer learning
\begin{itemize}
    \item Task A and B have same input $x$
    \item A lot more data for Task A than Task B
    \item Low level features from Task A could be helpful for Task B
\end{itemize}

\paragraph{Example 1: Cat recognition --- radiology diagnosis}
The following neural network is trained for cat recognition, but we want to adapt it for radiology
diagnosis. The neural network will learn about the structure and the nature of images. This initial
phase of training on image recognition is called pre-training, since it will pre-initialize the
weights of the neural network. Updating all the weights afterwards is called fine-tuning.

\begin{figure}[htb]
    \centering
    \includegraphics[width=45em]{figures/transfer-learning}
    \caption{Transfer learning example, from Task cats recognition to Task radiology diagnosis.}
    \label{fig:transfer-learning}
\end{figure}

For cat recognition
\begin{itemize}
    \item Input $x$: image
    \item Output $y$: 1---cat; 0---non-cat
\end{itemize}

For Radiology diagnosis
\begin{itemize}
    \item Input $x$: Radiology images --- CT Scan, X-rays
    \item Output $y$: Radiology diagnosis, 1---tumor malign; 0---tumor benign
\end{itemize}

\paragraph{Guideline}
\begin{itemize}
    \item Delete last layer of neural network
    \item Delete weights feeding into the last output layer of the neural network
    \item Create a new set of randomly initialized weights for the last layer only
    \item New dataset $(x, y)$
\end{itemize}

\subsubsection{Multi-task learning}
Multi-task learning refers to having one neural network do simultaneously several tasks.

When to use multi-task learning
\begin{itemize}
    \item Training on a set of tasks that could benefit from having shared lower-level features
    \item Usually: Amount of data you have for each task is quite similar
    \item Can train a big enough neural network to do well on all tasks
\end{itemize}

\paragraph{Example: Simplified autonomous vehicle}
The vehicle has to detect simultaneously several things: pedestrians, cars, road signs, traffic
lights, cyclists, etc. We could have trained four separate neural networks, instead of train one to
do four tasks. However, in this case, the performance of the system is better when one neural
network is trained to do four tasks than training four separate neural networks since some of the
earlier features in the neural network could be shared between the different types of objects.

The input $x^{(i)}$ is the image with multiple labels. The output $y^{(i)}$ has 4 labels which are
represents:
$$ \Vector{y^{(i)}} = \left[\begin{array}{c} 0 \\ 1 \\ 1 \\ 0 \end{array}\right]
\left.\begin{array}{l}
\text{Pedestrians} \\ \text{Cars} \\ \text{Road signs - Stop} \\ \text{Traffic lights}
\end{array}\right. \quad \longrightarrow \quad
\Matrix{Y} = \left[\begin{array}{cccc}
\vRule & \vRule & \vRule & \vRule \\
\Vector{y^{(1)}} & \Vector{y^{(2)}} & \Vector{y^{(3)}} & \Vector{y^{(4)}} \\
\vRule & \vRule & \vRule & \vRule \end{array}\right] \qquad
\Matrix{Y} \in \Set{R}^{(4, m)} $$

To train this neural network, the cost function is defined as follow:
$$ -\frac{1}{m} \sum_{i=1}^m \sum_{j=1}^4 (y_j^{(i)} \log(\hat{y}_j^{(i)})) + (1-y_j^{(i)})
\log(1-\hat{y}_j^{(i)})) $$

Also, the cost can be compute such as it is not influenced by the fact that some entries are not
labeled. For example:
$$ \Matrix{Y} = \left[\begin{array}{cccc} 1 & 0 & ? & ? \\ 0 & 1 & ? & 0 \\ 0 & 1 & ? & 1 \\
? & 0 & 1 & 0 \end{array}\right] $$

\subsection{End-to-end deep learning}
\subsubsection{What is end-to-end deep learning?}
End-to-end deep learning is the simplification of a processing or learning systems into one neural
network.

\paragraph{Example: Speech recognition model}
\begin{figure}[htb]
    \centering
    \includegraphics[width=45em]{figures/end-to-end-learning}
    \caption{Different ways to train a speech recognition model.}
    \label{fig:end-to-end-learning}
\end{figure}

End-to-end deep learning cannot be used for every problem since it needs a lot of labeled data. It
is used mainly in audio transcripts, image captures, image synthesis, machine translation, steering
in self-driving cars, etc.

\subsubsection{Whether to use end-to-end deep learning}
Before applying end-to-end deep learning, you need to ask yourself the following question: Do you
have enough data to learn a function of the complexity needed to map $x$ and $y$?

\paragraph{Pros:}
\begin{itemize}
    \item Let the data speak
    \begin{itemize}
        \item By having a pure machine learning approach, the neural network will learn from $x$ to
        $y$. It will be able to find which statistics are in the data, rather than being forced to
        reflect human preconceptions.
    \end{itemize}
    \item Less hand-designing of components needed
    \begin{itemize}
        \item It simplifies the design work flow.
    \end{itemize}
\end{itemize}

\paragraph{Cons:}
\begin{itemize}
    \item Large amount of labeled data
    \begin{itemize}
        \item It cannot be used for every problem as it needs a lot of labeled data.
    \end{itemize}
    \item Excludes potentially useful hand-designed component
    \begin{itemize}
        \item Data and any hand-design's components or features are the 2 main sources of knowledge
        for a learning algorithm. If the dataset is small, a hand-design system is a way to give
        manual knowledge into the algorithm.
    \end{itemize}
\end{itemize}


\end{document}
